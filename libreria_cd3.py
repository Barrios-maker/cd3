# -*- coding: utf-8 -*-
"""Libreria_CD3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t_qBKjqHIwXarcKKkEA04Z_YzV4WRnv1
"""

'''
Libreria_CD3: En esta libreria se encuentran una serie de clases utiles en la
ciencia de datos desarrolladas durante el cursado de la mateia
"ciencia de datos 3" de la Licenciatura en ciencia de Datos.

--------------------------------------------------------------------------------
Clases:

  ResumenNumerico
  ResumenGrafico
  RegresionLinealSimple
  RegresionLinealMultiple
  RegresionLogistica
  TestDeBondad
'''

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
import random

from scipy.stats import norm
from scipy.stats import t
from scipy.stats import chi2




class ResumenNumerico():
  '''
  Esta clase sirve para hacer un primer analisis exploratorio de los datos,
  recibe como entrada una areglo de numeros y genrea un resumen que contiene
  la media, la mediana, el desvio estandar, los cuartiles, el minimo y el maximo.
  '''
  def __init__(self, datos):
    self.datos = list(datos)

  def calculo_de_media(self):
    s = 0
    for i in self.datos:
      s += i
    media = s/len(self.datos)
    return media

  def calculo_de_mediana(self):
    datos = np.array(self.datos)
    mediana = np.median(datos)
    return mediana

  def calculo_de_desvio_estandar(self):
    m = self.calculo_de_media()
    desvio_estandar = ((sum([(i-m)**2 for i in self.datos]))/len(self.datos))**0.5
    return desvio_estandar

  def calculo_de_cuartiles(self):
    datos_ordenados = sorted(np.array(self.datos))

    q1 = np.percentile(datos_ordenados,25)
    q2 = np.percentile(datos_ordenados,50)
    q3 = np.percentile(datos_ordenados,75)

    return [q1, q2, q3]

  def generacion_resumen_numerico(self):
    res_num = {
    'Media': self.calculo_de_media(),
    'Mediana': self.calculo_de_mediana(),
    'Desvio': self.calculo_de_desvio_estandar(),
    'Cuartiles': self.calculo_de_cuartiles(),
    'Mínimo': min(self.datos),
    'Máximo': max(self.datos)
    }

    return res_num

  def muestra_resumen(self):
    res_num = self.generacion_resumen_numerico()
    for estad, valor in res_num.items():
      print(f"{estad}: {np.round(valor,3)}")




class ResumenGrafico():
  '''
  Esta clase sirve para hacer un primer analisis exploratorio de los datos,
  se instancia con un arreglo de datos y luego te permite estimar su densidad
  de probabilidad de varias maneras, ademas tambien tiene la opcion de generar
  datos con distribucion normal o con distribucion Bart Simpson.

  ------------------------------------------------------------------------------
  Metodos:
    generacion_del_histograma
    generar_datos_normales
    generar_datos_BS
    evaluar_histograma
    estimar_densidad
  '''
  def __init__(self,datos):
    self.datos = np.array(datos)

  def generacion_del_histograma(self,h):
    '''
    Este Metodo recibe un valor numrico h y genera un histograma con ancho de
    bins h. Devuelve dos listas numericas, bins y alturas, bins contien los
    los puntos en los que empiezan y termninan los bins y alturas contiene
    las alturas de cada bin.
    '''
    val_min = min(self.datos)
    val_max = max(self.datos)
    bins = np.arange(val_min, val_max, h)
    if bins[-1] < val_max :
      np.append(bins, bins[-1] + h )

    m = len(bins)
    alturas = [0 for i in range(m-1)]
    for i in self.datos:
      for j in range(len(bins)-1):
        if i == bins[0]:
          alturas[0] += 1
          break
        elif bins[j] < i < bins[j+1]:
          alturas[j] += 1
          break
    for i in range(len(alturas)):
      alturas[i] /= (len(self.datos) * h)

    return bins, alturas

  def generar_datos_normales(media,desvio,n):
    '''
      Este metodo recibe tres valores, media, devio y n. devulve n datos
      aleatorios con distribucion normal con media y desvio dados.
    '''
    return np.random.normal(media,desvio,n)

  def generar_datos_BS(media, desvio, n):
    '''
      Este metodo recibe tres valores, media, devio y n. devulve n datos
      aleatorios con distribucion Bart Simpson generados a partir de una normal
      con media y desvio dados.
    '''
    u = np.random.uniform(size=(n,))
    y = u.copy()
    ind = np.where(u > 0.5)[0]
    y[ind] = np.random.normal(media, desvio, size=len(ind))
    for j in range(5):
        ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
        y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))
    return y

  def evaluar_histograma(self,h,x):
    '''
    Este metodo recibe dos valores h y x. Con h llama al metodo
    generacion_del_histograma. x es un vector de numeros, el metodo devuelve un
    vector que le asigan a cada componente de x su altura en el histograma. Este
    metodo puede ser utilzado para estimar la funcion de densidad de probabilidad
    de los datos.
    '''
    bins, alturas = self.generacion_del_histograma(h)
    res = [0 for i in range(len(x))]

    for j in range(len(x)):
      if x[j] == min(self.datos):
        res[j] == alturas[0]
      else:
        for i in range(len(bins)-1):
          if bins[i]<x[j]<=bins[i+1]:
            res[j]=alturas[i]
            break
    return res

# --------Distribuciones--------

  def gaussiano(x):
    valor_kernel_gaussiano = np.exp(-(np.power(x,2))/2)/(np.sqrt(np.pi*2))
    return valor_kernel_gaussiano

  def uniforme (x):
    valor_kernel_uniforme = 0
    if -1/2 <= x < 1/2:
      valor_kernel_uniforme = 1
    return valor_kernel_uniforme

  def triangular(x):
    if -1<=x<0:
      return 1+x
    elif 0<=x<1:
      return 1-x
    else:
      return 0


  def cuadratico(x):
    if -1 <= x < 1:
      return 3/4*(1-np.power(x,2))
    else:
      return 0

# ------------------------------

  def estimar_densidad(self,x,h,kernel=uniforme):
    '''
      Este metodo recibe dos valores numericos x y h devuelve una estimacion de
      la probabilidad de x dados los datos originales, para esto realiza una
      estimacion de nucleo que depende del parametro h, el nucleo puede ser
      uniforme, gaussiano, triangular o cuadratico.
    '''
    frecuencia = 0
    for i in self.datos:
      frecuencia += kernel((x-i)/h)
    return frecuencia/(len(self.datos)*h)



class Regresion():
  '''
    Clase Regresion: Clase general para regresiones lineales multiples y
    logisticas se puede instanciar con dos argumentos, X e y, donde X son los
    datos predictores e y son los datos de la respuesta. Tambine se puede
    instanciar con 4 argumnetos, X, y, xt y yt donde los primeros dos son los
    datos de entrenamiento y los ultimos dos son los datos para testeo.

    ----------------------------------------------------------------------------
    Metodos:
      Grafica
  '''
  def __init__(self,X,y,Xt=None,yt = None):
    self.X = pd.DataFrame(X)
    self.y = y
    self.X_t = pd.DataFrame(Xt)
    self.y_t = yt

  def Grafica(self):
    '''
      Solo en caso de que sea posible este metodo realiza la grafica de la
      variable respuesta en funcion de la predictora.
    '''
    self.ajustar()
    if self.X.shape[1] != 1:
      return None
    else:
      plt.scatter(self.X,self.y, color = 'green', label = 'Datos')
      plt.plot(self.X,self.curva_ajustada, color = 'black', label = 'Ajuste')
      plt.legend()


class RegresionLinealSimple(Regresion):
  '''
  Clase RegresionLinealSimple: Clase para regresion lineal simple, se puede
  instanciar con dos argumentos, X e y, donde X es un vector con los datos de la
  variable predictora e y son los datos de la variable respuesta. Tambien se
  puede instanciar con 4 argumentos, X, y, xt y yt donde los primeros dos son
  los  datos de entrenamiento y los ultimos dos son los datos para testeo.
  --------------------------------------------------------------------------------
  Metodos:
    Grafica
    estimacion_betas
    ajustar
    residuos
    desvio
    analisis_residuos
    predecir
    testear
  '''
  def __init__(self,X,y,X_t = None,y_t = None):
    super().__init__(X,y,X_t,y_t)
    self.X = np.array(X)
    self.y = np.array(y)
    self.len = len(X)

  def estimacion_betas(self):
    '''
    Este metodo supone el modelo y = b0 + b1*x y devuelve los parametros
    b0 y b1 que mejor ajustan este modelo.
    '''
    mediax = np.mean(self.X)
    mediay = np.mean(self.y)
    num = sum([(self.X[i] - mediax)*(self.y[i]-mediay) for i in range(self.len)])
    denom = sum([(self.X[i]-mediax)**2 for i in range(self.len)])

    b1 = num/denom
    b0 = mediay - b1*mediax

    setattr(self,'parametros',[b0,b1])
    return self.parametros

  def ajustar(self):
    '''
    Este metodo asigna a cada valor de la variable predictora x un valor teorico
    bajo el modelo de regresin lineal simple y = b0 + b1*x y establece un nuevo
    atributo de tipo vector llamado curva_ajustada.
    '''
    self.estimacion_betas()
    b0 = self.parametros[0]
    b1 = self.parametros[1]
    y_teorico = [b0 +b1*i for i in self.X]
    setattr(self,'curva_ajustada',y_teorico)


  def residuos(self):
    '''
    Este metodo devuelve un vector con los residuos de la regresion lineal
    simple, es decir la diferencia entre el valor teorico y el valor real.
    '''
    self.estimacion_betas()
    b0 = self.parametros[0]
    b1 = self.parametros[1]
    r = [self.y[i]-(b0 + b1*self.X[i]) for i in range(self.len)]
    setattr(self,'res',r)
    return r

  def desvio(self):
    '''
    Este metodo devuelve el desvio de la regresion lineal simple.
    '''
    self.residuos()
    r = self.res
    return sum([i**2 for i in r])/(self.len-2)

  def analisis_residuos(self):
    '''
    ESte metodo imprime un qq plot de los residuos de la regresion vs una
    una distribucion normal teorica. esto sirve para chequear la hiotesis de
    normalidad  de los residuos. ademas imprime la variable preditora vs los
    residuos para chequear graficamente el supuesto de homocedasticidad.
    '''
    r = self.residuos()
    r_std = sorted((r-np.mean(r))/np.std(r))
    r_teoricos = norm.ppf([(0.5+i)/(self.len) for i in range(self.len)])
    plt.scatter(r_teoricos,r_std, color = 'blue', marker = 'o')
    plt.plot(r_teoricos,r_teoricos, color = 'red')
    plt.show()
    plt.scatter(self.X,r)
    plt.show()

  def predecir(self,x_nuevo,alpha = 0.05, IC = True):
    '''
    Este metodo predice el valor dque deberia tomar la vairable respuesta para
    un nuevo valor x bajo el modelo de regresion lineal simple, ademas devuelve
    un intervalo de confianza en el valor predicho. un segundo parametro alpha
    indica el nivel de significancia del intervalo de confianza, por defecto
    alpha = 0.05.
    '''
    self.estimacion_betas()
    b0 = self.parametros[0]
    b1 = self.parametros[1]
    prediccion = b0 +b1*x_nuevo
    desvio = self.desvio()

    if IC:
      var_b1=desvio/(sum((self.X-np.mean(self.X))**2))
      var_b0=desvio*np.sum(self.X**2)/(self.len*sum((self.X-np.mean(self.X))**2))
      cov_01=-np.mean(self.X)*desvio/sum((self.X-np.mean(self.X))**2)
      SE2=var_b0+(x_nuevo**2)*var_b1+2*x_nuevo*cov_01
      SE = np.sqrt(SE2)
      IC = [prediccion + t.ppf(alpha/2, self.len-2)*SE, prediccion - t.ppf(alpha/2, self.len-2)*SE]
      print('Prediccion: ', prediccion,'\n',' Intervalo de confianza del '+str((1-alpha)*100)+ ' % :', IC)
    else:
      return prediccion


  def testear(self):
    '''
    Este metodo ajusta el modelo con los datos de entrenamiento y luego intenta
    predecir los datos proporcionados para el testeo. Devuelve el error cuadratico
    medio entre los valores predichos y los reales proporcionados(yt).
    '''
    self.ajustar()
    X = self.X_t
    predicciones = []
    for i in X:
      predicciones.append(self.predecir(i, IC = False))

    dif = [predicciones[i] - self.y_t[i] for i in range(len(predicciones))]

    return('ECM = '+str(round((np.linalg.norm(dif)**2)/len(predicciones),3)))


class RegresionLinealMultiple(Regresion):
  """
  Clase RegresionLinealMultiple: Clase para regresión lineal múltiple, se puede
  instanciar con dos argumentos, X e y, donde X es un DataFrame con los datos de
  las variables predictoras e y son los datos de la variable respuesta. También
  se puede instanciar con 4 argumentos, X, y, xt y yt donde los primeros dos son
  los datos de entrenamiento y los últimos dos son los datos para testeo.
  --------------------------------------------------------------------------------
  Métodos:
    ajustar
    predecir
    residuos
    betas
    intercepcion
    testear
    qqplot

  """

  def __init__(self,x,y,xt = None, yt = None):
    super().__init__(x,y,xt,yt)
    self.x = x
    self.y = y
    self.model = None
    self.results = None

  def ajustar(self):
    '''
      Este método ajusta el modelo de regresión lineal múltiple a los datos de
      entrenamiento y establece un nuevo atributo llamado 'curva_ajustada' con
      los valores teóricos ajustados.
    '''
    X = sm.add_constant(self.x)
    self.model = sm.OLS(self.y, X)
    self.results = self.model.fit()
    y_teorico = self.results.predict(X)

  def predecir(self,x):
    '''
    Este método recibe un nuevo conjunto de datos x y devuelve las predicciones
    bjo el modelo de regresión lineal múltiple ajustado.
    '''
    self.ajustar()
    x = np.array(x)
    X = sm.add_constant(x)
    return self.results.predict(X)

  def residuos(self):
    '''
    Este método devuelve un vector con los residuos de la regresión lineal
    múltiple, es decir, la diferencia entre el valor teórico y el valor real.
    '''
    self.ajustar()
    betas = self.results.params

    r = [self.y[i]-(np.dot(betas[1:], self.x.loc[i]) + betas[0] ) for i in range(self.x.shape[0])]
    setattr(self,'res',r)
    return r

  def betas(self):
    '''
    Este método devuelve un vector con los coeficientes de la regresión lineal
    múltiple ajustado.
    '''
    return self.results.params[1:]

  def intercepcion(self):
    '''
    Este método devuelve el valor de la intersección de la regresión lineal
    múltiple ajustado.
    '''
    return self.results.params[0]

  def testear(self):
    '''
    Este método ajusta el modelo con los datos de entrenamiento y luego intenta
    predecir los datos proporcionados para el testeo. Devuelve el error
    cuadrático medio entre los valores predichos y los reales proporcionados (yt).
    '''
    self.ajustar()

    X = self.X_t
    y = np.array(self.y_t)
    n_test = self.X_t.shape[0]

    y_pred = self.predecir(X)

    error = [y_pred[i] - y[i] for i in range(n_test)]

    return ('ECM = '+str(round(((np.linalg.norm(error))**2)/n_test,3)))


  def qqplot(self):
    '''
      Este método imprime un qq plot de los residuos de la regresión versus una
    distribución normal teórica, lo cual sirve para chequear la hipótesis de
    normalidad de los residuos.
    '''
    self.ajustar()
    r = self.residuos()
    r_std = sorted((r-np.mean(r))/np.std(r))

    r_teoricos = norm.ppf([(0.5+i)/(self.x.shape[0]) for i in range(self.x.shape[0])])
    plt.scatter(r_teoricos,r_std, color = 'blue', marker = 'o')
    plt.plot(r_teoricos,r_teoricos, color = 'red')
    plt.show()




class RegresionLogistica(Regresion):
  '''
  Clase RegresionLogistica: Clase para regresión logística, se puede instanciar
  con dos argumentos, X e y, donde X es un DataFrame con los datos de las
  variables predictoras e y son los datos de la variable respuesta. También se
  puede instanciar con 4 argumentos, X, y, xt y yt donde los primeros dos son los
  datos de entrenamiento y los últimos dos son los datos para testeo.
  --------------------------------------------------------------------------------
  Métodos:
    ajustar
    predecir
    testear
    Ajustar_pc
    tabla
  '''
  def __init__(self,X,y,Xt = None, yt = None):
    super().__init__(X,y,Xt,yt)
    self.X = pd.DataFrame(X)
    self.y = y
    self.desvios = None

  def proba(self,L_var):
    '''
    Este método recibe un nuevo conjunto de datos L_var y devuelve la
    probabilidad predicha bajo el modelo de regresión logística ajustado.
    '''
    L_var = np.array(L_var)
    L_var = np.insert(L_var,0,1)
    termino_lineal = np.dot(self.parametros,L_var)
    l = np.exp(termino_lineal)/(1+np.exp(termino_lineal))
    return 1-l

  def ajustar(self):
    '''
    Este método ajusta el modelo de regresión logística a los datos de
    entrenamiento y establece un nuevo atributo llamado 'curva_ajustada' con los
    valores teóricos ajustados.
    '''
    matriz_diseño = sm.add_constant(self.X)
    modelo = sm.Logit(self.y,matriz_diseño)
    resultado = modelo.fit()
    print(resultado.summary())
    setattr(self,'parametros', resultado.params)

    if self.X.shape[1] == 1:
      dominio = np.linspace(self.X.min(),self.X.max(),self.X.shape[0])
      crv_aj = [self.proba(i) for i in dominio]
      setattr(self,'curva_ajustada', crv_aj)

  def testear(self,pc = 0.5):
    '''
      Este método ajusta el modelo con los datos de entrenamiento y luego intenta
      predecir los datos proporcionados para el testeo. Devuelve una tabla de
      confusión con las predicciones y los valores reales.
    '''

    X = pd.DataFrame(self.X_t)
    y = self.y_t
    n_test = len(self.X_t)

    #self.ajustar()

    y_logistica = [1-self.proba(X.iloc[i]) for i in range(X.shape[0])]
    y_pred = [int(y_logistica[i] >= pc) for i in range(X.shape[0])]


    # Tabla de confucion

    Y_Y = [(y_pred[i] == 1) and (y[i] == 1) for i in range(n_test)]
    N_Y = [(y_pred[i] == 0) and (y[i] == 1) for i in range(n_test)]
    Y_N = [(y_pred[i] == 1) and (y[i] == 0) for i in range(n_test)]
    N_N = [(y_pred[i] == 0) and (y[i] == 0) for i in range(n_test)]

    tabla =  pd.DataFrame({
    'y_test=1': [sum(Y_Y), sum(N_Y)],
    'y_test=0': [sum(Y_N), sum(N_N)],

    }, index=['y_pred=1', 'y_pred=0'])

    setattr(self,'Tabla',tabla)

    especificidad = tabla['y_test=0'][1]/(tabla['y_test=0'][1]+tabla['y_test=0'][0])
    sensibilidad = tabla['y_test=1'][0]/(tabla['y_test=1'][1]+tabla['y_test=1'][0])

    setattr(self,'Especificidad',especificidad)
    setattr(self,'Sensibilidad',sensibilidad)



  def Ajustar_pc(self,precision = 100, graficos = False):
    '''
      Este método ajusta el punto de corte (pc) óptimo para la predicción mediante
      la maximización del índice de Youden. Devuelve el valor óptimo de pc y,
      opcionalmente, grafica la especificidad y sensibilidad en función de pc.
    '''
    p = np.linspace(0,1,precision)
    especificidades = []
    sensibilidades = []
    for i in p:
      pred = self.testear(i)
      especificidades.append(self.Especificidad)
      sensibilidades.append(self.Sensibilidad)

    aux = [sensibilidades[i]+especificidades[i]-1 for i in range(len(p))]
    for i in range(len(aux)):
      youden = 0
      if aux[i] == max(aux):
        youden = p[i]
        break

    self.testear(pc = youden)
    setattr(self,'Youden',youden)

    if graficos:
      plt.plot(p,[sensibilidades[i]+especificidades[i]-1 for i in range(len(p))], color = 'black', label = 'Youden = ' +str(round(youden,4)))
      plt.plot([youden], [max(aux)], color = 'red')
      plt.scatter(p,especificidades, color = 'coral', label = 'especificidad')
      plt.scatter(p,sensibilidades, color = 'skyblue', label = 'sensibilidad')
      plt.legend()



  def tabla(self):
    '''
    Este método devuelve la tabla de confusión obtenida del método testear.
    '''
    return self.Tabla




class TestDeBondad():
  '''
  Clase TestDeBondad: Clase para realizar el test de bondad de ajuste usando el
  estadístico chi-cuadrado. Este test evalúa si una muestra sigue una distribución
  de probabilidad específica.
  --------------------------------------------------------------------------------
  Test de hipótesis:
    H_0: La muestra sigue la distribución de probabilidad.
    H_1: La muestra no sigue la distribución de probabilidad.

  Parámetros:
    prob: Lista de probabilidades teóricas.
    muestra: Lista con los datos de la muestra observada.
  --------------------------------------------------------------------------------
  Métodos:
    testear
  '''
  def __init__(self,prob,muestra):
    self.prob = prob
    self.muestra = muestra
    self.n = sum(muestra)

  def testear(self,alpha = 0.05):
    '''
    Método para realizar el test de chi-cuadrado para la bondad de ajuste.
    Calcula el estadístico observado, el crítico y el p-valor. Determina si se
    rechaza la hipótesis nula con el nivel de significancia dado.

    Parámetros:
      alpha: Nivel de significancia para el test (default=0.05).

    Imprime:
      - Estadístico observado (Z observado).
      - Estadístico crítico (Z crítico).
      - p-valor.
      - Conclusión sobre la hipótesis nula.
    '''
    esperados = [self.prob[i]*self.n for i in range(len(self.muestra))]

    Z_obs = sum([((esperados[i]-self.muestra[i])**2)/esperados[i] for i in range(len(self.prob))])
    df = len(self.prob)-1

    Z_crit = chi2.ppf(1-alpha,df)

    print('Z observado:',Z_obs,'Z critico:',Z_crit,'\n')

    p_valor = chi2.sf(Z_obs,df)

    print('p-valor:',p_valor,'\n')

    if p_valor < alpha:
      print('Se rechaza H0 \n')
      print('Podemos estar mas de un '+str((1-alpha)*100)+'% seguros de que la muestra no sigue la distribucion')
    else:
      print('No se puede rechazar H0 con un nivel del '+str(alpha*100)+'% de significancia \n')
      print('No me atreveria a negar que la muestra en efecto siga la distribucion de probabilidad')



def separardatos(df,p = 0.2):
  n_test = int(df.shape[0]*p)
  i_test = random.sample(range(0, df.shape[0]), n_test)

  datos_test = df.iloc[i_test].copy()

  datos_train = df.drop(i_test).copy()

  return datos_train.reset_index(drop = True), datos_test.reset_index(drop = True)